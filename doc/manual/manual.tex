\documentclass[10pt,letterpaper,oneside,openany,showtrims,draft]{memoir}
\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{csquotes}
\usepackage{unicode-math}
\usepackage{xcolor}
\usepackage[os=win]{menukeys}
% \usepackage{fontspec}
\usepackage{siunitx}
\usepackage[final=true]{hyperref}           % should be loaded last since it redefines commands

\definecolor{stateActive}{HTML}{448844}
\definecolor{stateBlockedGC}{HTML}{9C6FD6}
\definecolor{stateBlockedHappensBefore}{HTML}{BB6363}
\definecolor{stateBlockedNet}{HTML}{BB5D5D}
\definecolor{stateBlockedSyscall}{HTML}{BA4F41}
\definecolor{stateBlocked}{HTML}{BA4141}
\definecolor{stateGC}{HTML}{9C6FD6}
\definecolor{stateInactive}{HTML}{888888}
\definecolor{stateReady}{HTML}{4BACB8}
\definecolor{stateStuck}{HTML}{000000}
\definecolor{stateMerged}{HTML}{B9BB63}
\definecolor{stateUserRegion}{HTML}{F2A2E8}
\definecolor{stateStack}{HTML}{79B579}
\definecolor{stateSampled}{HTML}{98D597}

\newcommand{\code}[1]{{\ttfamily\mbox{#1}}}
\newcommand{\noun}[1]{{\emph{#1}}}
\newcommand{\traceState}[2]{\fcolorbox{black}{#2}{#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\shortcut}{\ctrl{} or \cmdmac}

\sisetup{group-separator={,}}

% \headstyles{memman}
\settrimmedsize{11in}{210mm}{*}
\setlength{\trimtop}{0pt}
\setlength{\trimedge}{\stockwidth}
\addtolength{\trimedge}{-\paperwidth}
\settypeblocksize{7.75in}{33pc}{*}
\setulmargins{4cm}{*}{*}
\setlrmargins{1.25in}{*}{*}
\setmarginnotes{17pt}{51pt}{\onelineskip}
\setheadfoot{\onelineskip}{2\onelineskip}
\setheaderspaces{*}{2\onelineskip}{*}
\setsidefeet{\marginparsep}{9em}%
   {\onelineskip}{0pt}%
   {\normalfont\footnotesize}{\textheight}%
\checkandfixthelayout

\AtBeginEnvironment{verbatim}{\microtypesetup{activate=false}}

\begin{document}
\frontmatter

\mainmatter

\microtypesetup{protrusion=false}
\tableofcontents
\microtypesetup{protrusion=true}

\chapter{Introduction}

The official implementation of the Go language depends on a runtime for memory management (via garbage collection) and for scheduling goroutines.
The runtime does its work in the background, out of sight, but it provides a powerful tool for inspecting its actions: the runtime tracer.

The tracer produces an exact (i.e., not sampled) stream of events describing in detail the execution of goroutines.
It shows when goroutines start or stop, where they block, what unblocks them, how long they spend stuck in syscalls, and more.
The trace also contains information about the work of the garbage collector (GC):
It indicates stop-the-world phases, the duration of mark phases, and even how much time each goroutine is spending assisting the GC.
In short, the runtime tracer provides a complete view of the interactions between the runtime and our code.

These traces can be useful in two situations:
When the runtime isn't behaving as we expect, negatively affecting the performance of our code,
and when we want to analyze how our goroutines interact with each other and the outside world.

Because the runtime is responsible for scheduling goroutines, it may make decisions that we don't agree with.
For example, it might take an unusually long time to schedule an important goroutine.
This might become apparent due to unreliable performance and large tail latencies.
When this happens, the runtime trace is the best way to make sure that the issue does indeed lie with Go and not our code.

More commonly, traces can be used to understand the behavior of our own code.
Virtually all interactions between goroutines go through the runtime.
Similarly, most interactions with the outside world (such as file or network I/O, or syscalls) go through the runtime, too.
Runtime traces, then, provide an accurate view into what our code is doing while it's not spending time on the CPU executing instructions.
In other words, they show us what our code is waiting on.
This, too, can help diagnose performance issues (this time caused by us), deadlocks, and more.

Unfortunately, the runtime tracer hasn't seen widespread adoption.
This can be attributed to shortcomings in the official frontend, the \emph{Trace viewer}.
The trace viewer isn't a custom-made tool for viewing Go runtime traces;
instead, it is a repurposed copy of Chrome's old tracing frontend, \emph{Catapult}.
Unfortunately, it wasn't made to handle the millions of events that can occur in even short runtime traces,
nor does it have any functionality that helps with understanding the specifics of the behavior of the Go runtime.
Finally, the UI is just weird, using controls that are neither ergonomic nor intuitive.
Go tries to work around these shortcomings, by splitting large traces into smaller ones and by clever use of Catapult's features.
However, workarounds can only do so much and come with their own problems.
For these reasons, the runtime tracer is usually people's last choice for debugging problems.

Gotraceui was created to address these problems.
It was written from scratch, with a focus on displaying large Go runtime traces and making them more accessible.
It can handle millions of events in a single trace, overlay traces with CPU profiling data as well as memory usage,
and annotate runtime traces with metadata extracted from stack traces, among other things.
Unlike the official frontend, Gotraceui provides both per-processor and per-goroutine timelines,
with the latter often being much more useful for understanding the behavior of user code.

Even though the current version of Gotraceui is still in its early stages,
lacking many of the envisioned features,
it has already proven to be a useful tool in everyday use
and people have diagnosed real problems with it without prior experience with runtime traces.

The purpose of this manual is not just to explain every aspect of Gotraceui,
but also to offer an introduction to runtime traces themselves,
and, where necessary, explain how the runtime works to make better sense of the traces it produces.

The author hopes that runtime traces will become a standard debugging tool for most Go developers.

\chapter{Quickstart}

\chapter{System requirements}

Gotraceui runs on Linux (X11 and Wayland), Windows, and macOS.

Runtime traces are very dense in information and can contain millions of events in the span of seconds.
The format emitted by \code{runtime/trace} is optimized for small and low overhead output and is highly compressed.
To be able to process and display a trace, Gotraceui has to parse and materialize it in memory.
Memory usage is roughly 30x the size of the input trace.
That is, a \qty{300}{\mega\byte} trace file will need about \qty{9}{GB} of memory to be loaded by Gotraceui.
For reference, an example \qty{300}{\mega\byte} trace file was produced by tracing a busy Prometheus instance for one minute,
resulting in \qty{66044021}{} events.
This represents an extreme example.
Many of your traces will be much smaller than that.
For example, tracing \code{net/http}'s tests produces a \qty{7.3}{\mega\byte} trace instead, 

\chapter{The user interface}
The following sections will describe the various components of Gotraceui's UI.

\section{Main window}
The main window displays a main menu and an interactive \noun{canvas} representing the trace data.
% TODO: \midaligned{{\externalfigure[images/screenshots/main_window.png][width=0.95\textwidth]}}
The canvas can be moved around by dragging with \keys{LMB} or by scrolling.
Holding \keys{\shortcut} while scrolling zooms in and out, centered around the cursor's position.
Dragging with \keys{\shortcut+LMB} selects a region of time to zoom to.
The \menu{Display} menu contains commands for changing the way \noun{timelines} are displayed on the canvas,
as well as commands for quick navigation.

\subsection{Axis}
The top of the window shows the \noun{axis}.
% TODO: \midaligned{{\externalfigure[images/screenshots/axis.png][width=0.95\textwidth]}}
The bold tick displays the absolute time at that point in the trace.
Ticks to the left and right of it show relative decrements and increments to this absolute time.

By default, the absolute tick is placed at the middle of the axis,
as analyzing traces often involves looking at what happend before and after an event.
The tick can be moved by clicking and dragging anywhere on the axis.
Alternatively, its context menu allows quickly placing it at the beginning, middle, or end of the axis.

\todo{talk about negative absolute ticks}

Additionally, the axis contains red and purple sections,
which correspond to the garbage collector's stop-the-world phase and general activity.
Pressing \keys{O} cycles through displaying the red section, both sections, or none of the sections across the entire canvas.

\subsection{Timelines, tracks, and spans}
The canvas consists of a number of horizontally stacked timelines.
A timeline might show a processor, a goroutine, or phases of the garbage collector.
Every timeline has a label, hovering over which may display a tooltip.

% TODO: \midaligned{{\externalfigure[images/screenshots/processor-tooltip.png][width=0.5\textwidth]}}

For example, for processors, the tooltip will show how much time was spent executing user code,
doing garbage collection work,
and being idle.
Pressing \keys{\shortcut+LMB} on a label will zoom the canvas such that all spans in that timeline are visible.
Pressing \keys{LMB} on a goroutine label will open a panel with additional information about the goroutine.

A timeline consists of one or more horizontally stacked \noun{tracks}.

% TODO: \midaligned{{\externalfigure[images/screenshots/activity.png][width=0.95\textwidth]}}

For example, the screenshot shows a goroutine timeline with \todo{NUM} tracks.
The first track contains the runtime tracing information,
while the other \todo{NUM} tracks represent the \todo{NUM} levels of call stacks acquired from events and CPU samples.
See \autoref{cpu-sampling} for more information on the specifics of CPU sampling in Gotraceui.
Another source of tracks is code that is annotated with user regions.
% TODO: actually show user regions in the screenshot

Each track consists of a series of \noun{spans}.
A span represents a state for some duration of time.
For example, a goroutine may be blocked on a channel send operation for \unit{100 ms}, and this would be displayed as a single span.
Hovering over a span will show context-specific information about it,
including its state and duration,
but also additional information such as tags
\todo{(link to the section on tags})
or the reason for being in a certain state.
Pressing \keys{LMB} on a span will open a list of events that happened during that span.
Pressing \keys{\shortcut+LMB} on a span will zoom the canvas such that the span is fully visible.

Spans have different colors depending on the states they represent.
Different kinds of timelines and tracks may use different color schemes.
\todo{link to where the color schemes are documented}

Depending on the zoom level, individual spans may be too small to display.
Gotraceui uses two strategies to ensure that spans are always visible.
First, it merges consecutive tiny spans, displaying them as one big span instead.
Such merged spans get their own color.
\todo{(which?)}
Zooming into merged spans unmerges them.
Second, if there aren't enough spans to merge, the span is given a minimum size.
\todo{mention how that means following spans might be rendered smaller than they are,
and that zooming into an enlarged span may not change its size for quite a while.}

\subsubsection{Span colors}
Spans in processor timelines will have one of two colors:
\traceState{Green}{stateActive} for spans that represent running user goroutines,
and \traceState{purple}{stateGC} for spans that represent garbage collection work.

Spans in the first track of goroutine timelines can have many different colors,
representing the many different states a goroutine can be in.
You can find an exhaustive list of all goroutine states --- and the corresponding span colors --- in \autoref{goroutine-states}
.

User regions are displayed in \traceState{light pink}{stateUserRegion}.
Stack traces are displayed either in a \traceState{light shade of green}{stateStack} if they're from events,
or in a \traceState{lighter shade of green}{stateSampled} if they're been acquired via CPU sampling.

When small spans get merged, they will be displayed in one of two ways:
if all merged spans have the same state,
then we will display a gradient from \traceState{light yellow}{stateMerged} to the color of the merged spans.
For merged spans with mixed states, a solid light yellow is shown instead.

\todo{talk about context menu}

\section{Goroutine windows}
\begin{itemize}
\item \todo{General information}
\item \todo{Stack trace}
\item \todo{Per-state statistics}
\item \todo{Events}
\end{itemize}

\section{Processor windows}
\todo{TODO}
\section{Heatmaps}
\todo{TODO}




\chapter{Tags}
Gotraceui will annotate spans with tags, which further describe the states goroutines are in.
These tags are produced by automatically parsing stack traces,
and for example deducing that a goroutine that's blocked on pollable I/O got to that state by making a TLS-encrypted
HTTP request over TCP,
which provides a lot more information than just \enquote{I/O}.

Being based on stack trace parsing, tags are provided on a best-effort basis.
Without a matching, hand-written pattern, tags will not be recognized.
The author of the software adds new patterns as he notices them and tries to keep them in sync with new releases of Go.

The following tags exist:

\begin{itemize}
\item HTTP, for I/O related to HTTP
\item TCP, for I/O related to TCP
\item TLS, for I/O related to TLS
\item accept, for blocking on \href{https://man7.org/linux/man-pages/man2/accept.2.html}{accept(2)}ing on a network connection
\item dial, for blocking on dialing a network connection
\item network, for network I/O
\item read, for read I/O
\end{itemize}

A single span can be annotated with multiple tags.

\chapter{The \code{runtime/trace} package}
\section{Adding tracing to your application}
\todo{TODO}

\section{User annotations}
\todo{TODO}

\section{CPU profiling}
\todo{TODO}

\section{\todo{Tracing in tests or something}}
\todo{TODO}

\chapter{The Go runtime}


\section{Introduction}
\todo{TODO}

\section{The scheduler}
Go programs can have very many goroutines, up to millions.
Because it wouldn't be feasible to map one goroutine to one OS level thread,
Go has to distribute goroutines over a smaller number of threads.
To do so, the scheduler has to decide which goroutines to run when,
part of which involves tracking which goroutines {\em can} run.
The activity related to this makes up a large part of what the runtime trace captures and Gotraceui visualizes.
It is thus helpful to understand how the scheduler works.

\subsection{Ms, Ps, and Gs}
\todo{
  M = OS threads
  P = token/internal state, GOMAXPROCS of them
  G = goroutines

  Many Gs run on many Ms, at most len(P) of them. Blocking syscalls etc get their own M.

  LockOSThread locks a G to an M; it's still free to jump between Ps.

  \url{https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit}
  \url{https://morsmachine.dk/go-scheduler}
}

\subsection{Syscalls}\label{syscalls}
\todo{TODO}

\subsection{Gosched}
\todo{TODO}

\subsection{Pollable vs. unpollable I/O}\label{netpoller}
\todo{TODO}

\section{Goroutine states}\label{goroutine-states}
\todo{
  Something about how a goroutine will be in one of several states.
  States have colors, some of them have labels, different labels at different zoom levels.
}

Some of these states are actual states in the Go runtime, while other states are introduced by Gotraceui to increase
the level of detail.

\begin{itemize}
\item \traceState{created}{stateReady} Newly created goroutines will be in this state before they get scheduled for the first time.
  It is a special case of the ready state.

\item \traceState{active}{stateActive} Active goroutines are those that are currently running.

\item \traceState{send}{stateBlockedHappensBefore}, \traceState{recv}{stateBlockedHappensBefore},
  \traceState{select}{stateBlockedHappensBefore} These states describe the three ways in which goroutines can be
  blocked on channel communication.

\item \traceState{sync}{stateBlockedHappensBefore} This state is used by goroutines that are blocked on sync
  primitives, such as \code{sync.Mutex}.

\item \traceState{sync.Once}{stateBlockedHappensBefore} Blocked on a \code{sync.Once}.
  This is a special case of the sync state and detected by Gotraceui based on stack traces.

\item \traceState{sync.Cond}{stateBlockedHappensBefore} Blocked on a condition variable (\code{sync.Cond}.)

\item \traceState{I/O}{stateBlockedNet} This state is entered by goroutines that are waiting for pollable I/O to complete.
  See \autoref{netpoller} for more information.

\item \traceState{syscall}{stateBlockedSyscall} Goroutines enter this state when they invoke a blocking syscall.
  See \autoref{syscalls} for an explanation of the difference between blocking and non-blocking syscalls in the context of Go.

\item \traceState{blocked}{stateBlocked} Blocked goroutines are waiting for something to happen, but we don't know what.
  This usually happens for goroutines of the runtime that don't emit more accurate information.
  User goroutines will usually have more specific states such as \enquote{send}.

\item \traceState{inactive}{stateInactive} This state is one of Gotraceui's custom states and is used for
  goroutines that are blocked or ready to run, but aren't actually eager to run.
  For blocked goroutines,
  this is exclusively used by goroutines of the runtime that block on some lock to pace the amount of work they do.
  Goroutines that are technically in the ready state but are marked inactive are those that called \code{runtime.Gosched} or \code{time.Sleep},
  as this indicates that they willingly gave up part of their share in CPU time,
  and their time spent waiting shouldn't be considered scheduler latency.

  \todo{We still need to describe this state}
\item \traceState{BlockedGC}{stateGC}

\item \traceState{ready}{stateReady} A goroutine in this state isn't blocked on anything anymore and can start running
  as soon as it gets scheduled.
  A goroutine can be in this state because there aren't any free Ps to run it,
  or simply because the scheduler hasn't gotten around to starting it yet.
  Goroutines can transition into this state from the active state if they get preempted,
  or from any of the various blocked states once they get unblocked.
  Time spent in this state is commonly called scheduler latency.

  \todo{We still need to describe these states}
\item \traceState{GC (idle)}{stateGC}

\item \traceState{GC (dedicated)}{stateGC}

\item \traceState{GC mark assist}{stateGC} Goroutines in the \enquote{GC mark assist} state are assisting the mark phase.
  See \autoref{gc} to learn more about the garbage collector.

\item \traceState{GC sweep}{stateGC} Goroutines in the \enquote{GC sweep} state are sweeping memory.
  See \autoref{gc} to learn more about the garbage collector.

  \todo{Drawing black on black isn't brilliant}

\item \traceState{stuck}{stateStuck} Goroutiens in this state are stuck and can never make progress.
  This happens, for example, when receiving from a nil channel or using \code{select} with no cases.

\end{itemize}

\section{Garbage collection}\label{gc}
Go uses a concurrent mark and sweep garbage collector.
Its activity will interact with the scheduling of your goroutines in various ways,
which we'll explore in this section.
We will focus on the details that matter for understanding runtime traces.
There are many more details to how the GC works
and you're encouraged to read the \href{https://go.dev/doc/gc-guide}{official documentation} to learn more about it.

\subsection{\todo{Something about mark, sweep, mark assist, STW, how GC gets triggered, etc}}

\section{The runtime's goroutines}
The runtime spawns several of its own goroutines that will show up in most traces.
Most of these exist to help with the concurrent garbage collector.

\begin{itemize}
\item \code{bgsweep} is a low priority goroutine that sweeps spans when there are idle Ps. This reduces the amount
  of sweeping that has to be done by other goroutines.
\item \todo{bgscavenger}
\item \todo{gcBgMarkWorker}
\item \code{forcegchelper} periodically gets woken up and forces a garbage collection cycle to start.
  This ensures that garbage gets collected regularly even if the program isn't allocating enough memory to hit the heap target.
\item \code{runfinq} is the goroutine that is responsible for running finalizers.
  That means that this runtime goroutine will execute code provided by the user via \code{runtime.SetFinalizer}.
\end{itemize}



\chapter{CPU sampling}\label{cpu-sampling}
% \intodo{We probably want to move this chapter into a subchapter of the UI chapter}
In addition to sequences of runtime events and user regions, Gotraceui can also display tracks of stack traces.
Such tracks can help spot macroscopic patterns in code execution.
They are reconstructed from CPU profiling samples as well as the stacks associated with certain runtime events and thus have a sampled,
imprecise nature.

% TODO: \midaligned{\clip[width=0.95\textwidth]{\externalfigure[images/screenshots/stack_samples.png]}}

It is important to understand the nature and limitations of the data displayed in these tracks.
Strictly speaking, each CPU profiling sample and each runtime event describes an instant in time, not a duration.
However, in our reconstruction, we connect consecutive stack traces,
making it look like a function has been running for the entire duration between two events or samples.
This is a lie.

A CPU profiling sample states that at a specific point in time, a certain function was executing.
It doesn't say anything about what happened right after the sample.
By default, samples occur at a frequency of \unit{100 Hz}, i.e., once every \unit{10 ms}.
This means that there is \unit{10 ms} of uncertainty after a sample.
The function might've returned anywhere from \unit{0} to \unit{10 ms} after the sample.
It may even have been called repeatedly.
All we really know is that at one point in time, the function was running.

Nevertheless, when looking at larger scales (hundreds of milliseconds, if not seconds),
especially when looking at repetitive executions,
this visualization can expose patterns.
The more often the same stack gets sampled, the more likely it is that it is indeed a major contributor to the code's execution.
However, the data is woefully inadequate at small scales --- the kind of scales at which runtime trace data exists.
You shouldn't rely on the sampled stack traces to fill in the gaps between two runtime events that happened \unit{100 us} apart.

Gotraceui combines both CPU profiling and some runtime events when reconstructing stack trace tracks.
CPU samples are those used by pprof.
Beginning with Go 1.19, runtime traces include CPU samples if profiling is enabled at the same time as tracing.
However, goroutines that are blocked don't receive any samples, so we additionally use
the stacks associated with runtime events that signal state transitions, such as goroutines calling \code{time.Sleep},
to fill in the gaps.
This way, the last stack before a potentially long time without samples will be as accurate as possible.

Using sampling and runtime events together further affects how you should treat durations in stack traces
because of the following two reasons:

\begin{enumerate}
\item We've already established that span durations derived from samples aren't accurate, but this is even more
  pronounced when trying to look at runtime events and the corresponding reconstructed tracks together. For
  example, when we enter the \enquote{blocked on channel send} state, we will display the stack associated with that
  event, until we get the next stack. This means, however, that even after we unblock and go back to the
  \enquote{ready} and \enquote{active} states, the stack will not update, as neither event contains a new stack. This
  will make it look like we're still in the \code{runtime.chansend} function even after we've completed the channel
  send. Only on the next sample or relevant event will the stack update. It is therefore important to either look at
  runtime events or the stacks, but not both together. Runtime events show an exact history of what happened in
  the runtime, while stacks show a guess at what happened in user code.

\item CPU profiling samples happen at a fairly constant rate, which means all samples have the same uncertainty.
  Runtime events, however, can happen at arbitrary points. If a sample is followed by a runtime event \unit{1 ms}
  later then it will look much smaller than if it were followed by a runtime event \unit{9 ms} later, even though in
  the latter case we still don't know what happened for the first \unit{9 ms}. This is another instance of span
  durations of samples not having much meaning.
\end{enumerate}

Gotraceui doesn't use the stacks of other events, such as syscalls, because this would lead to bias in the data.
Samples are evenly distributed and – on average – lead to a fair representation of a program's execution, and
considering state transitions is necessary, as we have explained earlier. Including other events, on the other hand,
would skew the displayed stacks towards those events.


\backmatter

\end{document}
